### base config ####
BASE: &BASE
    # data_dir: '/ocean/projects/phy220045p/jakhar/py2d_dealias/results/Re5000_fkx4fky4_r0.1_b20.0/NoSGS/NX64/dt0.0002_IC1/data/'
    data_dir: '/ocean/projects/phy220045p/jakhar/py2d_dealias/results/Re5000_fkx4fky4_r0.1_b20.0/NoSGS/NX256/dt0.0002_IC1/data/'

    # Info of runs and directories
    exp_dir: '/ocean/projects/phy220045p/jakhar/2d_emulator_vision/SSL-for-2D-Turbulence/results/'

    # Login for Weights and biases
    log_to_screen: !!bool True
    log_to_wandb: !!bool True
    project: '2DTurb-FouRKS' # 
    group: 'base-emulator'
    name: 'base'

    diagnostic_logs: !!bool True

    fresh_start: True # Training from checkpoint vs start (Need to test if it resumes at checkpoint)
    early_stopping: !!bool False # Not implemented

    save_checkpoint: !!bool True # Two checkoiunts to save model weights: best and last epoc

    train_file_range: [200000, 220000]
    valid_file_range: [300000, 302000]
    # train_file_range: [20000, 40000]
    # valid_file_range: [40000, 42000]
    batch_size: 32 # Global batch size (not per GPU)
    target_step: 3 # Number of time steps to predict in the future (prediction time step = target_step * saved time step)
    num_workers: 2 # Number of workers for data loader per GPU (no imporvement after 2)
    pin_memory: !!bool True


    gpu: !!bool True
    lr: 5e-4
    scheduler: 'ReduceLROnPlateau' # 'CosineAnnealingLR' 'ReduceLROnPlateau'
    warmup: !!bool True # Warmup for LR scheduler (Start with low LR and then increase)
    warmup_startfactor: 0.001 
    warmup_totaliters: 3 # Warm-up learning rate for the first N epochs
    weight_decay: 1e-6 # Regularization term for loss function (L2 norm)
    max_epochs: 400
    checkpointing: !!bool False # True : For large models gradinet checkpointing is used to save memory (slows down upto 2 or so) May need to increase for patch size = 2

    integrator: null # 'null' 'E1' 'RK2' 'RK4' None, Euler 1st order, Runge-Kutta 2nd order, Runge-Kutta 4th order

    spectral_loss: !!bool False # True: Use spectral loss for training
    spectral_loss_weight: 1 # Weight (Regularizer Coefficient) for spectral loss
    spectral_loss_threshold_wavenumber: 50 # Threshold wavenumber for spectral loss

    img_size: 256
    patch_size: 4
    num_frames: 1 # Number of frames to consider for input (t, t-\deltat, t-2\deltat)
    tubelet_size: 1 # Number of frames to consider for input imbedding - multiple timesteps are included in the same frame
    in_chans: 2 # Number of input channels (u, v)
    encoder_embed_dim: 96 #192
    encoder_depth: 4 #6
    encoder_num_heads: 4 #6
    decoder_embed_dim: 96 #192
    decoder_depth: 4 #6
    decoder_num_heads: 4 #6
    mlp_ratio: 4.
    num_out_frames: 1 # Numebr of output frames (time steps) to predict
    patch_recovery: 'linear' # linear conv subpixel_conv

    train_tendencies: !!bool False # Training neural netwrok on difference of X(t+dt) - X(t) = Tendencies

    mae_finetune: False
    # mae_finetune_fp: '/scratch/user/u.dp200518/ML_Weights/MAE_Pretrain/TS0_NF1_LRSNone_200epochs_small.pt'
    # freeze_layers: ['encoder']

